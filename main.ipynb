{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30d209e9-fe16-42b0-92ff-857b66f81a77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /opt/anaconda3/lib/python3.13/site-packages (8.3.191)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /opt/anaconda3/lib/python3.13/site-packages (from ultralytics) (2.1.3)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from ultralytics) (3.10.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from ultralytics) (4.12.0.88)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /opt/anaconda3/lib/python3.13/site-packages (from ultralytics) (11.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/anaconda3/lib/python3.13/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /opt/anaconda3/lib/python3.13/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/anaconda3/lib/python3.13/site-packages (from ultralytics) (1.15.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in /opt/anaconda3/lib/python3.13/site-packages (from ultralytics) (2.8.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /opt/anaconda3/lib/python3.13/site-packages (from ultralytics) (0.23.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.13/site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/anaconda3/lib/python3.13/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: polars in /opt/anaconda3/lib/python3.13/site-packages (from ultralytics) (1.33.0)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from ultralytics) (2.0.16)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.13/site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.13/site-packages (from requests>=2.23.0->ultralytics) (2025.6.15)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch>=1.8.0->ultralytics) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.13/site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.13/site-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd7b005-3b39-42e7-a92e-0b161f406f25",
   "metadata": {},
   "source": [
    "–ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∫–∏ –∏–∑ —Ñ–∞–π–ª–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "244d0012-e9be-4d21-84f4-6df54297abf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        filepath   label\n",
      "0    images/train/0695_bluzy.jpg   bluzy\n",
      "1   images/train/9524_bryuki.jpg  bryuki\n",
      "2    images/train/7283_bluzy.jpg   bluzy\n",
      "3  images/train/11326_bryuki.jpg  bryuki\n",
      "4    images/train/7760_bluzy.jpg   bluzy\n",
      "label\n",
      "bluzy     7074\n",
      "bryuki    6402\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–∞–±–ª–∏—Ü–∞–º–∏ –∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏\n",
    "import os  # –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –æ—Å—è–º–∏\n",
    "\n",
    "train_dir = 'images/train/'\n",
    "train_filepaths = [os.path.join(train_dir, f) for f in os.listdir(train_dir)]   # —Å–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "\n",
    "labels = []\n",
    "for path in train_filepaths:\n",
    "    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∫—É –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞\n",
    "    label = os.path.basename(path).split('_')[1].split('.')[0]\n",
    "    labels.append(label)\n",
    "\n",
    "train_df = pd.DataFrame({'filepath': train_filepaths, 'label': labels})  # –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –ø—É—Ç–µ–π —Ñ–∞–π–ª–æ–≤ –∏ –º–µ—Ç–æ–∫\n",
    "\n",
    "# –ü–æ—Å–º–æ—Ç—Ä–∏–º, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–æ—Å—å –∏ —Å–∫–æ–ª—å–∫–æ —É –Ω–∞—Å –∫–ª–∞—Å—Å–æ–≤\n",
    "print(train_df.head())\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc5146-5938-486d-a070-d4f0f5c6f0b1",
   "metadata": {},
   "source": [
    "–ü—Ä–æ–ø–∏—Å—ã–≤–∞–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11b32c2c-c35d-4ce8-82a5-1ddbf88f3b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ (—Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(), # –°–ª—É—á–∞–π–Ω–æ–µ –æ—Ç—Ä–∞–∂–µ–Ω–∏–µ\n",
    "    transforms.RandomRotation(10),     # –°–ª—É—á–∞–π–Ω—ã–π –ø–æ–≤–æ—Ä–æ—Ç\n",
    "    transforms.ToTensor(),             # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "])\n",
    "\n",
    "# –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ (–±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏!)\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1595c7c6-669e-4a88-bff9-dafc8295fd37",
   "metadata": {},
   "source": [
    "–†–∞–∑–±–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –∏ —Ç–µ—Å—Ç–æ–≤—ã–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c611ca96-1078-4205-baec-3f54b0680d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞—à DataFrame –Ω–∞ –¥–≤–µ —á–∞—Å—Ç–∏: 80% –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ, 20% –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é\n",
    "train_subset_df, val_subset_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.2,       # –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "    random_state=42,     # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    stratify=train_df['label'] # –û—á–µ–Ω—å –≤–∞–∂–Ω–æ! –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de5b567-4401-4e25-adb3-82021f304949",
   "metadata": {},
   "source": [
    "–ö–ª–∞—Å—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ —Ç–µ–Ω–∑–æ—Ä–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee2a68b0-c645-4fa5-960f-d5497c20d9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class ClothingDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        \"\"\"\n",
    "        –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä –∫–ª–∞—Å—Å–∞.\n",
    "        :param dataframe: pandas DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ 'filepath' –∏ 'label'.\n",
    "        :param transform: –∫–æ–Ω–≤–µ–π–µ—Ä —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π (–Ω–∞—à transforms.Compose).\n",
    "        \"\"\"\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –º–µ—Ç–æ–∫ –≤ —á–∏—Å–ª–∞\n",
    "        self.label_map = {'bluzy': 0, 'bryuki': 1}\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ.\"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –û–î–ò–ù –æ–±—ä–µ–∫—Ç (–∫–∞—Ä—Ç–∏–Ω–∫—É –∏ –º–µ—Ç–∫—É) –ø–æ –µ–≥–æ –∏–Ω–¥–µ–∫—Å—É.\n",
    "        –≠—Ç–æ —Å–∞–º–∞—è –≤–∞–∂–Ω–∞—è —á–∞—Å—Ç—å!\n",
    "        \"\"\"\n",
    "        # 1. –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏ –º–µ—Ç–∫—É –∏–∑ DataFrame\n",
    "        image_path = self.df.iloc[idx]['filepath']\n",
    "        label_str = self.df.iloc[idx]['label']\n",
    "\n",
    "        # 2. –û—Ç–∫—Ä—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ PIL\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # 3. –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–∞—à –∫–æ–Ω–≤–µ–π–µ—Ä —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image)\n",
    "\n",
    "        # 4. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—É—é –º–µ—Ç–∫—É –≤ —á–∏—Å–ª–æ\n",
    "        label_int = self.label_map[label_str]\n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —á–∏—Å–ª–æ –≤ —Ç–µ–Ω–∑–æ—Ä PyTorch\n",
    "        label_tensor = torch.tensor(label_int, dtype=torch.long)\n",
    "\n",
    "        return image_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4b3896-bcf1-440b-a5b9-dea6b845f942",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã –∏ –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "710d12d7-2a82-49a4-a8e6-153c048e1b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä Dataset –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏, –ø–µ—Ä–µ–¥–∞–µ–º –µ–º—É –Ω–∞—à DataFrame –∏ train_transforms\n",
    "train_dataset = ClothingDataset(train_subset_df, transform=train_transforms)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä Dataset –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏, –ø–µ—Ä–µ–¥–∞–µ–º –µ–º—É val_transforms (–±–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏!)\n",
    "val_dataset = ClothingDataset(val_subset_df, transform=test_transforms) # –ò—Å–ø–æ–ª—å–∑—É–µ–º test_transforms\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º DataLoader'—ã\n",
    "# shuffle=True –¥–ª—è –æ–±—É—á–∞—é—â–µ–≥–æ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ - –æ—á–µ–Ω—å –≤–∞–∂–Ω–æ! –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É.\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False) # –î–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –Ω–µ –Ω—É–∂–Ω–æ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af67b56-6cf6-4435-beaf-129762eea4aa",
   "metadata": {},
   "source": [
    "–ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å YOLO11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d286d6e2-43f4-42d3-8d4e-42ff1b8652d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/Users/sergei/Library/Application Support/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-cls.pt to 'yolo11m-cls.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 22.4/22.4MB 2.2MB/s 10.1s<0.0s\n",
      "YOLO11m-cls summary: 106 layers, 11,634,216 parameters, 0 gradients, 40.6 GFLOPs\n",
      "(106, 11634216, 0, 40.6263808)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolo11m-cls.pt')\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∏–º –º–æ–¥–µ–ª—å\n",
    "print(model.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82230702-6524-4f42-862a-2e55cc237363",
   "metadata": {},
   "source": [
    "–ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ, –ø–æ–Ω—è—Ç–Ω–æ–π YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ecadffb-b2c9-42d3-abc6-2dafe346de62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def prepare_yolo_folders(df, base_dir):\n",
    "    for idx, row in df.iterrows():\n",
    "        label = row['label']\n",
    "        src = row['filepath']\n",
    "        dst_dir = os.path.join(base_dir, label)\n",
    "        os.makedirs(dst_dir, exist_ok=True)\n",
    "        shutil.copy(src, dst_dir)\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫\n",
    "os.makedirs('lamoda_dataset/train', exist_ok=True)\n",
    "os.makedirs('lamoda_dataset/val', exist_ok=True)\n",
    "\n",
    "prepare_yolo_folders(train_subset_df, 'lamoda_dataset/train')\n",
    "prepare_yolo_folders(val_subset_df, 'lamoda_dataset/val')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d020c8-cbc8-4a23-a80a-88fca5633124",
   "metadata": {},
   "source": [
    "–ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ YOLO11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c378e3bc-f8d3-42fb-9863-80b642e28519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.13/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.13/site-packages (0.23.0)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.8.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.13/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.13/site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torchaudio-2.8.0-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: torchaudio\n",
      "Successfully installed torchaudio-2.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd5ef4c8-7b75-4525-a524-ad65ea01680b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.191 üöÄ Python-3.13.5 torch-2.8.0 MPS (Apple M4 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=lamoda_dataset, degrees=0.0, deterministic=True, device=mps, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11m-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/classify/train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /Users/sergei/Documents/Lamoda images classification/lamoda_dataset/train... found 10780 images in 2 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /Users/sergei/Documents/Lamoda images classification/lamoda_dataset/val... found 2696 images in 2 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 10                  -1  1    660482  ultralytics.nn.modules.head.Classify         [512, 2]                      \n",
      "YOLO11m-cls summary: 106 layers, 10,355,778 parameters, 10,355,778 gradients, 39.6 GFLOPs\n",
      "Transferred 296/296 items from pretrained weights\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 84.9¬±103.8 MB/s, size: 26.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/sergei/Documents/Lamoda images classification/lamoda_dataset/train... 10780 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 10780/10780 34436098.3it/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 4.9¬±1.8 MB/s, size: 1.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/sergei/Documents/Lamoda images classification/lamoda_dataset/val... 2696 images, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2696/2696 10748900.7it/s 0.0s0s\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 49 weight(decay=0.0), 50 weight(decay=0.0005), 50 bias(decay=0.0)\n",
      "Image sizes 224 train, 224 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/classify/train2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       1/10      3.36G     0.1004         28        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 337/337 2.8it/s 2:02<0.4ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 43/43 3.7it/s 11.7s0.2s\n",
      "                   all      0.993          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       2/10      3.32G    0.08931         28        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 337/337 3.0it/s 1:54<0.3ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 43/43 4.3it/s 10.1s0.2s\n",
      "                   all      0.988          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       3/10      3.31G    0.08368         28        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 337/337 2.9it/s 1:55<0.3ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 43/43 4.3it/s 10.0s0.2s\n",
      "                   all      0.986          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       4/10      3.31G    0.06172         28        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 337/337 3.0it/s 1:54<0.3ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 43/43 4.3it/s 10.0s0.2s\n",
      "                   all      0.994          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       5/10      3.31G    0.03652         28        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 337/337 3.0it/s 1:53<0.3ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 43/43 4.3it/s 10.1s0.2s\n",
      "                   all      0.992          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       6/10      3.31G    0.03376         28        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 337/337 3.0it/s 1:53<0.3ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 43/43 4.3it/s 10.0s.2s\n",
      "                   all      0.996          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       7/10      3.31G     0.0273         28        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 337/337 3.0it/s 1:53<0.3ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 43/43 4.3it/s 10.1s0.2s\n",
      "                   all      0.995          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       8/10      3.31G    0.02241         28        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 337/337 3.0it/s 1:53<0.3ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 43/43 4.3it/s 9.9s0.2s\n",
      "                   all      0.996          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K       9/10      3.31G    0.01672         28        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 337/337 2.9it/s 1:56<0.3ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 43/43 4.3it/s 10.0s0.2s\n",
      "                   all      0.997          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "\u001b[K      10/10      3.31G    0.01369         28        224: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 337/337 2.9it/s 1:57<0.3ss\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 43/43 4.3it/s 9.9s0.2s\n",
      "                   all      0.997          1\n",
      "\n",
      "10 epochs completed in 0.349 hours.\n",
      "Optimizer stripped from runs/classify/train2/weights/last.pt, 20.9MB\n",
      "Optimizer stripped from runs/classify/train2/weights/best.pt, 20.9MB\n",
      "\n",
      "Validating runs/classify/train2/weights/best.pt...\n",
      "Ultralytics 8.3.191 üöÄ Python-3.13.5 torch-2.8.0 MPS (Apple M4 Pro)\n",
      "YOLO11m-cls summary (fused): 57 layers, 10,344,194 parameters, 0 gradients, 39.3 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /Users/sergei/Documents/Lamoda images classification/lamoda_dataset/train... found 10780 images in 2 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /Users/sergei/Documents/Lamoda images classification/lamoda_dataset/val... found 2696 images in 2 classes ‚úÖ \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "\u001b[K               classes   top1_acc   top5_acc: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 43/43 4.6it/s 9.4s0.2s\n",
      "                   all      0.997          1\n",
      "Speed: 0.0ms preprocess, 0.3ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns/classify/train2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
       "\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x422460e60>\n",
       "curves: []\n",
       "curves_results: []\n",
       "fitness: 0.9987017810344696\n",
       "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
       "results_dict: {'metrics/accuracy_top1': 0.9974035620689392, 'metrics/accuracy_top5': 1.0, 'fitness': 0.9987017810344696}\n",
       "save_dir: PosixPath('runs/classify/train2')\n",
       "speed: {'preprocess': 0.04486422770212229, 'inference': 0.309693093102407, 'loss': 4.1713799074988605e-06, 'postprocess': 1.6937684457754655e-05}\n",
       "task: 'classify'\n",
       "top1: 0.9974035620689392\n",
       "top5: 1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ\n",
    "import torch\n",
    "\n",
    "model.train(\n",
    "    data='lamoda_dataset',  # –ü–∞–ø–∫–∞ —Å train –∏ val\n",
    "    epochs=10,\n",
    "    imgsz=224,  # –ú–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å 224 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏\n",
    "    batch=32,\n",
    "    device='mps'\n",
    ")\n",
    "\n",
    "# –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª—å –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ runs/classify/trainX/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "864d53d3-f68b-49e8-a15a-a9fafd9c9ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ‚ö†Ô∏è \n",
      "inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
      "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤: 3369, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: 3369\n",
      "–§–∞–π–ª submission.csv –≥–æ—Ç–æ–≤!\n"
     ]
    }
   ],
   "source": [
    "# –ü–∞–ø–∫–∞ —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\n",
    "test_dir = 'images/test/'\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "results = model.predict(source=test_dir, imgsz=224, device='mps', verbose=False)\n",
    "\n",
    "# –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "file_names = []\n",
    "predictions = []\n",
    "\n",
    "for r in results:\n",
    "    file_names.append(os.path.basename(r.path))        # –∏–º—è —Ñ–∞–π–ª–∞\n",
    "    cls_id = int(r.probs.top1)                         # –∏–Ω–¥–µ–∫—Å –∫–ª–∞—Å—Å–∞\n",
    "    predictions.append(r.names[cls_id])                # –∏–º—è –∫–ª–∞—Å—Å–∞ (bluzy –∏–ª–∏ bryuki)\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—ã —Å–ø–∏—Å–∫–æ–≤\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤: {len(file_names)}, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {len(predictions)}\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º DataFrame\n",
    "submission = pd.DataFrame({'index': file_names, 'label': predictions})\n",
    "\n",
    "# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ id, —á—Ç–æ–±—ã –ø–æ—Ä—è–¥–æ–∫ —Å–æ–≤–ø–∞–¥–∞–ª —Å Kaggle\n",
    "submission = submission.sort_values('index')\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"–§–∞–π–ª submission.csv –≥–æ—Ç–æ–≤!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc9114-1708-456e-9aec-84b1469f350c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
